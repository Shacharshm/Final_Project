{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1937,"status":"ok","timestamp":1652519522690,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-180},"id":"NHVVJvIuKn-h","outputId":"73a15c37-91d7-447a-b03c-b5c4351e9cc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1652519522691,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-180},"id":"ErsKsbyJzSKs"},"outputs":[],"source":["import os\n","\n","## shachar\n","dirPath = \"/content/drive/MyDrive/Colab Notebooks/Final project/Experiments\"\n","## ariel\n","# dirPath = \"/content/drive/MyDrive/Colab Notebooks/Experiments\"\n","\n","os.chdir(dirPath)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2631,"status":"ok","timestamp":1652519525314,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-180},"id":"OMsytu0iKtje","outputId":"39d0926e-4e9d-45a5-e13a-4027bac44896"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"]}],"source":["!pip install tensorflow-addons"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3958,"status":"ok","timestamp":1652519529266,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-180},"id":"WIKUFenjKtmD"},"outputs":[],"source":["from Data_extraction_transformer import Get_Data\n","from Results import Get_Results\n","\n","import collections\n","import logging\n","import os\n","import pathlib\n","import re\n","import string\n","import sys\n","import time\n","import math\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import backend as K\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1652519529267,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-180},"id":"VJu3ScdoKtpI"},"outputs":[],"source":["K_SEED = 330\n","\n","class args:\n","\n","  def __init__(self,input_data,roi,net,roi_name,zscore,train_size):\n","    self.input_data = input_data\n","    self.roi = roi\n","    self.net = net\n","    self.roi_name = roi_name\n","    self.K_RUNS = K_RUNS\n","    # preprocessing\n","    self.zscore = zscore\n","    # training parameters\n","    self.train_size = train_size\n","\n","# data parameters\n","args.input_data = 'data/roi_ts'\n","args.roi = 300\n","args.net = 7\n","args.roi_name = 'roi'\n","args.K_RUNS = 4\n","# preprocessing\n","args.zscore = 1\n","# training parameters\n","args.train_size = 100"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":416,"status":"ok","timestamp":1652519529675,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-180},"id":"z9zuc5ihpBVi","outputId":"f70c0aeb-6d8e-414c-d5e3-14c12a914cf9"},"outputs":[{"output_type":"stream","name":"stdout","text":["number of classes = 15\n"]}],"source":["def _get_clip_labels():\n","    '''\n","    assign all clips within runs a label\n","    use 0 for testretest\n","    '''\n","    # where are the clips within the run?\n","    timing_file = pd.read_csv('data/videoclip_tr_lookup.csv')\n","\n","    clips = []\n","    for run in range(args.K_RUNS):\n","        run_name = 'MOVIE%d' %(run+1) #MOVIEx_7T_yz\n","        timing_df = timing_file[timing_file['run'].str.contains(run_name)]  \n","        timing_df = timing_df.reset_index(drop=True)\n","\n","        for jj, row in timing_df.iterrows():\n","            clips.append(row['clip_name'])\n","            \n","    clip_y = {}\n","    jj = 1\n","    for clip in clips:\n","        if 'testretest' in clip:\n","            clip_y[clip] = 0\n","        else:\n","            clip_y[clip] = jj\n","            jj += 1\n","\n","    return clip_y\n","\n","clip_y = _get_clip_labels()\n","k_class = len(np.unique(list(clip_y.values())))\n","print('number of classes = %d' %k_class)\n","\n","clip_names = np.zeros(k_class).astype(str)\n","clip_names[0] = 'testretest'\n","for key, item in clip_y.items():\n","    if item!=0:\n","        clip_names[item] = key"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":189809,"status":"ok","timestamp":1652519722261,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-180},"id":"6CmFX2zQKtwP","outputId":"080ee516-34de-41a2-8604-5110710dd010"},"outputs":[{"output_type":"stream","name":"stdout","text":["---\n","roi\n","---\n","---\n","roi\n","---\n","loading run 1/4\n","loading run 2/4\n","loading run 3/4\n","loading run 4/4\n","data loading time: 162.63 seconds\n","number of subjects = 176\n","number of features = 300\n","number of classes = 15\n","seq lengths = [ 84 245 222 189  65 227 260 250 181 186 205 143 233 231 256]\n"]}],"source":["X_train, train_len, y_train, X_val, val_len, y_val, X_test, test_len, y_test, train_list, test_list, clip_time = Get_Data(args)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":270,"status":"ok","timestamp":1652519739555,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-180},"id":"iDPQndEQCTBD","outputId":"2c0d922a-8dd8-4087-a256-d79784c67f74"},"outputs":[{"output_type":"stream","name":"stdout","text":["vis:\n","train: (1800, 260, 47)\n","val: (684, 260, 47)\n","test: (684, 260, 47)\n","SomMot:\n","train: (1800, 260, 56)\n","val: (684, 260, 56)\n","test: (684, 260, 56)\n","Attn:\n","train: (1800, 260, 68)\n","val: (684, 260, 68)\n","test: (684, 260, 68)\n","limbic:\n","train: (1800, 260, 20)\n","val: (684, 260, 20)\n","test: (684, 260, 20)\n","Cont:\n","train: (1800, 260, 40)\n","val: (684, 260, 40)\n","test: (684, 260, 40)\n","DMN:\n","train: (1800, 260, 68)\n","val: (684, 260, 68)\n","test: (684, 260, 68)\n"]}],"source":["def GetNetwork(X_train, X_val, X_test,startLH,endLH,startRH,endRH):\n","  X_train_LH = X_train[:,:,startLH:endLH]\n","  X_train_RH = X_train[:,:,startRH:endRH]\n","  X_train_end = tf.concat([X_train_LH, X_train_RH], axis=2)\n","\n","  X_val_LH = X_val[:,:,startLH:endLH]\n","  X_val_RH = X_val[:,:,startRH:endRH]\n","  X_val_end = tf.concat([X_val_LH, X_val_RH], axis=2)\n","\n","  X_test_LH = X_test[:,:,startLH:endLH]\n","  X_test_RH = X_test[:,:,startRH:endRH]\n","  X_test_end = tf.concat([X_test_LH, X_test_RH], axis=2)\n","\n","  return X_train_end, X_val_end, X_test_end\n","\n","networksDict = {'vis':{'startLH':0,'endLH':24,'startRH':151,'endRH':174,'train':[], 'val':[], 'test':[]},\n","                'SomMot':{'startLH':24,'endLH':53,'startRH':174,'endRH':201,'train':[], 'val':[], 'test':[]},\n","                'Attn':{'startLH':53,'endLH':85,'startRH':201,'endRH':237,'train':[], 'val':[], 'test':[]},\n","                'limbic':{'startLH':85,'endLH':95,'startRH':237,'endRH':247,'train':[], 'val':[], 'test':[]},\n","                'Cont':{'startLH':95,'endLH':112,'startRH':247,'endRH':270,'train':[], 'val':[], 'test':[]},\n","                'DMN':{'startLH':112,'endLH':150,'startRH':270,'endRH':300,'train':[], 'val':[], 'test':[]},\n","                'full':{'train':X_train, 'val':X_val, 'test':X_test},\n","                'fullShuffled':{'train':X_train, 'val':X_val, 'test':X_test},\n","                'fullOffset':{'train':X_train, 'val':X_val, 'test':X_test}\n","                }\n","\n","for net in networksDict:\n","  if net == 'full' or net == 'fullShuffled' or net == 'fullOffset': continue\n","  networksDict[net]['train'], networksDict[net]['val'], networksDict[net]['test'] = GetNetwork(X_train, X_val, X_test, networksDict[net]['startLH'], networksDict[net]['endLH'], networksDict[net]['startRH'], networksDict[net]['endRH'])\n","  print(net + ':')\n","  print('train: '+ str(networksDict[net]['train'].shape))\n","  print('val: '+ str(networksDict[net]['val'].shape))\n","  print('test: '+ str(networksDict[net]['test'].shape))"]},{"cell_type":"markdown","metadata":{"id":"sFKlhtoyJhlJ"},"source":["Encoder build"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":361,"status":"ok","timestamp":1652519743031,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-180},"id":"sYEkiGxfdGJo"},"outputs":[],"source":["from tensorflow.keras.layers import MultiHeadAttention\n","\n","def lr_scheduler(epoch, lr, warmup_epochs=13, decay_epochs=100, initial_lr=1e-6, base_lr=1e-4, min_lr=5e-5):\n","    if epoch <= warmup_epochs:\n","        pct = epoch / warmup_epochs\n","        return ((base_lr - initial_lr) * pct) + initial_lr\n","\n","    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n","        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n","        return ((base_lr - min_lr) * pct) + min_lr\n","\n","    return min_lr\n","\n","class Time2Vec(keras.layers.Layer):\n","    def __init__(self, kernel_size=1):\n","        super(Time2Vec, self).__init__(trainable=True, name='Time2VecLayer')\n","        self.k = kernel_size\n","    \n","    def build(self, input_shape):\n","        # trend\n","        self.wb = self.add_weight(name='wb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n","        self.bb = self.add_weight(name='bb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n","        # periodic\n","        self.wa = self.add_weight(name='wa',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n","        self.ba = self.add_weight(name='ba',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n","        super(Time2Vec, self).build(input_shape)\n","    \n","    def call(self, inputs, **kwargs):\n","        bias = self.wb * inputs + self.bb\n","        dp = K.dot(inputs, self.wa) + self.ba\n","        wgts = K.sin(dp) # or K.cos(.)\n","\n","        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n","        ret = K.reshape(ret, (-1, inputs.shape[1]*(self.k+1)))\n","        return ret\n","    \n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], input_shape[1]*(self.k + 1))\n","\n","class AttentionBlock(keras.Model):\n","    def __init__(self, name='AttentionBlock', num_heads=2, head_size=128, ff_dim=None, dropout=0, **kwargs):\n","        super().__init__(name=name, **kwargs)\n","\n","        if ff_dim is None:\n","            ff_dim = head_size\n","\n","        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)\n","        self.attention_dropout = keras.layers.Dropout(dropout)\n","        self.attention_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.ff_conv1 = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')\n","        # self.ff_conv2 at build()\n","        self.ff_dropout = keras.layers.Dropout(dropout)\n","        self.ff_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","    def build(self, input_shape):\n","        self.ff_conv2 = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1) \n","\n","    def call(self, inputs):\n","        x, attention_scores = self.attention(inputs, inputs, return_attention_scores=True)\n","        x = self.attention_dropout(x)\n","        x = self.attention_norm(inputs + x)\n","\n","        x = self.ff_conv1(x)\n","        x = self.ff_conv2(x)\n","        x = self.ff_dropout(x)\n","\n","        x = self.ff_norm(inputs + x)\n","        return x, attention_scores\n","\n","class ModelTrunk(keras.Model):\n","      def __init__(self, classes, inputs, name='ModelTrunk', time2vec_dim=1, num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0, **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.MaskingLayer = keras.layers.Masking(mask_value=0.0, input_shape = [None, inputs.shape[-1]])\n","        self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n","        if ff_dim is None:\n","            ff_dim = head_size\n","        self.dropout = dropout\n","        self.classes = classes\n","        self.attention_layers = [AttentionBlock(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, dropout=dropout) for _ in range(num_layers)]\n","        self.dense_layer = keras.layers.Dense(units = 512, activation = 'relu')\n","        self.dropout_layer = keras.layers.Dropout(dropout)\n","        self.final_layer = tf.keras.layers.Dense(classes, activation='softmax')\n","\n","        \n","      def call(self, inputs):\n","        #time_embedding = keras.layers.TimeDistributed(self.time2vec)(inputs)\n","        #x = K.concatenate([inputs, time_embedding], -1)\n","        #x = MaskingLayer(inputs)\n","        x = inputs\n","        for attention_layer in self.attention_layers:\n","            x, attention_scores = attention_layer(x)\n","        x = self.dense_layer(x)\n","        x = self.dropout_layer(x)\n","        x = self.final_layer(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":292,"status":"ok","timestamp":1652520310449,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-180},"id":"0UCh_n-6dGSN"},"outputs":[],"source":["EPOCHS = 45\n","num_layers = [4, 6, 8]\n","d_model = 300\n","dff = 260\n","key_dim = 260\n","num_heads = [1, 4, 6, 8]\n","dropout_rate = 0.1\n","BUFFER_SIZE = 20000\n","BATCH_SIZE = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsBpm0aBeEyL"},"outputs":[],"source":["import pickle\n","from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n","\n","## vis: for heads:[6 8] missing layers: 4\n","## limbic for heads: 8 missing layers: 4\n","## DMN: for heads: 8 missing layers: [4 6]\n","\n","def make_batches(ds):\n","  return (\n","      ds\n","      .cache()\n","      .shuffle(BUFFER_SIZE)\n","      .batch(BATCH_SIZE)\n","      .prefetch(tf.data.AUTOTUNE))\n","\n","def GetCmat(model, X, y):\n","  y_hat_hold = model.predict(X)\n","  y_hat = np.argmax(y_hat_hold, axis=2)\n","  true_y = y\n","\n","  y_overtime = []\n","  y_hat_overtime = []\n","  for rows_y,rows_y_hat in zip(true_y,y_hat):\n","    values, counts = np.unique(rows_y, return_counts=True)\n","    ind = np.argmax(counts)\n","    if values[ind] == 15:\n","      counts[ind] = 0\n","      ind = np.argmax(counts)\n","    y_overtime.append(values[ind])\n","\n","    values, counts = np.unique(rows_y_hat, return_counts=True)\n","    ind = np.argmax(counts)\n","    if values[ind] == 15:\n","      counts[ind] = 0\n","      ind = np.argmax(counts)\n","    y_hat_overtime.append(values[ind])\n","\n","  cm = confusion_matrix(y_overtime,y_hat_overtime)\n","  acc = accuracy_score(y_overtime,y_hat_overtime)\n","  return cm, acc\n","\n","def printLoop(net, head, layer):\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print(f'---------------------------NET: {net}  number of heads: {head} number of layers: {layer}--------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","\n","\n","for net in networksDict:\n","  if net == 'fullOffset' or net == 'fullShuffled': continue\n","  for heads in num_heads:\n","    for layers in num_layers:\n","      printLoop(net, heads, layers)\n","      train_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['train'],y_train))\n","      val_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['val'],y_val))\n","      input = networksDict[net]['train']\n","      train_batch = make_batches(train_data)\n","      val_batch = make_batches(val_data)\n","\n","      myModel = ModelTrunk(inputs=input, name=f'Model_{net}_numHeads_{heads}', time2vec_dim=1, num_heads=heads, head_size=key_dim, ff_dim=dff, num_layers=layers, dropout=dropout_rate, classes=(len(clip_time)+1))\n","\n","      myModel.compile(\n","          optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","          loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","          metrics=[keras.metrics.SparseCategoricalAccuracy()]\n","      )\n","\n","      callbacks = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=0)\n","\n","      # EarlyStopping criteria\n","      early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n","\n","      history = myModel.fit(train_batch, validation_data=val_batch, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callbacks, early_stopping])\n","\n","      # results directory\n","      RES_DIR = f'/content/drive/MyDrive/Colab Notebooks/Final project/Experiments/results/encoder/{net}'\n","      if not os.path.exists(RES_DIR):\n","          os.makedirs(RES_DIR)\n","\n","      # summarize history for accuracy\n","      fig = plt.figure()\n","      plt.plot(history.history['sparse_categorical_accuracy'])\n","      plt.plot(history.history['val_sparse_categorical_accuracy'])\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n accuracy')\n","      plt.ylabel('accuracy')\n","      plt.xlabel('epoch')\n","      plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","      fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} accuracy', dpi=fig.dpi)\n","\n","      # summarize history for loss\n","      fig = plt.figure()\n","      plt.plot(history.history['loss'])\n","      plt.plot(history.history['val_loss'])\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n loss')\n","      plt.ylabel('loss')\n","      plt.xlabel('epoch')\n","      plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","      fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} loss', dpi=fig.dpi)\n","\n","      ## val Cmat\n","      cm, acc = GetCmat(myModel, networksDict[net]['val'], y_val)\n","      fig = plt.figure()\n","      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","      disp.plot()\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n val acc: {acc:.5}')\n","      plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat val', dpi=fig.dpi)\n","      plt.show()\n","\n","      ## test Cmat\n","      cm, acc = GetCmat(myModel, networksDict[net]['test'], y_test)\n","      fig = plt.figure()\n","      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","      disp.plot()\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n test acc: {acc:.5}')\n","      plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat test', dpi=fig.dpi)\n","      plt.show()\n","\n","\n","      results, results_prob = Get_Results(args, myModel, networksDict[net]['train'], y_train, train_list, train_len, networksDict[net]['test'], y_test, test_list, test_len, clip_time)\n","\n","      myModel.save(f'/content/drive/MyDrive/Colab Notebooks/Final project/Experiments/models/encoder_models/{net}/Model_{net}_numHeads_{heads}_num_layers_{layers}')\n","\n","      res_path = (RES_DIR + \n","                  '/%s_%d_net_%s' %(args.roi_name, args.roi, net) +\n","                  '_k_layers_%d' %(layers) +\n","                  '_heads_%d_batch_size_%d' %(heads, BATCH_SIZE) +\n","                  '_num_epochs_%d.pkl' %(EPOCHS))\n","\n","      # save results\n","      with open(res_path, 'wb') as f:\n","          pickle.dump([results, results_prob], f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jRMxgBj0KkO"},"outputs":[],"source":["### offset and shuffled\n","\n","import pickle\n","from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n","\n","def make_batches(ds):\n","  return (\n","      ds\n","      .cache()\n","      .shuffle(BUFFER_SIZE)\n","      .batch(BATCH_SIZE)\n","      .prefetch(tf.data.AUTOTUNE))\n","\n","def GetCmat(model, X, y):\n","  y_hat_hold = model.predict(X)\n","  y_hat = np.argmax(y_hat_hold, axis=2)\n","  true_y = y\n","\n","  y_overtime = []\n","  y_hat_overtime = []\n","  for rows_y,rows_y_hat in zip(true_y,y_hat):\n","    values, counts = np.unique(rows_y, return_counts=True)\n","    ind = np.argmax(counts)\n","    if values[ind] == 15:\n","      counts[ind] = 0\n","      ind = np.argmax(counts)\n","    y_overtime.append(values[ind])\n","\n","    values, counts = np.unique(rows_y_hat, return_counts=True)\n","    ind = np.argmax(counts)\n","    if values[ind] == 15:\n","      counts[ind] = 0\n","      ind = np.argmax(counts)\n","    y_hat_overtime.append(values[ind])\n","\n","  cm = confusion_matrix(y_overtime,y_hat_overtime)\n","  acc = accuracy_score(y_overtime,y_hat_overtime)\n","  return cm, acc\n","\n","def printLoop(net, head, layer):\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print(f'---------------------------NET: {net}  number of heads: {head} number of layers: {layer}--------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","\n","def ShuffleAndOffset(y):\n","  y_shuffle = tf.random.shuffle(y)\n","  y_offset = np.array(y)\n","  for i,_ in enumerate(y):\n","    j = np.where(y[i,:]>0)\n","    y_offset[i,j] += 2\n","    j = np.where(y_offset[i,:]==15)\n","    y_offset[i,j] = 1\n","    j = np.where(y_offset[i,:]==16)\n","    y_offset[i,j] = 2\n","    j = np.where(y_offset[i,:]==17)\n","    y_offset[i,j] = 15\n","  y_offset = tf.convert_to_tensor(y_offset)\n","  return y_shuffle, y_offset\n","\n","y_shuffled_t, y_offset_t = ShuffleAndOffset(y_train)\n","\n","# num_layers = [6]\n","# num_heads = [1]\n","\n","for net, y_train_copy in zip(['fullOffset'], [y_offset_t]): # in zip(['fullShuffled', 'fullOffset'], [y_shuffled_t, y_offset_t]):\n","  for heads in num_heads:\n","    for layers in num_layers:\n","      printLoop(net, heads, layers)\n","      train_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['train'],y_train_copy))\n","      val_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['val'],y_val))\n","      input = networksDict[net]['train']\n","      train_batch = make_batches(train_data)\n","      val_batch = make_batches(val_data)\n","\n","      myModel = ModelTrunk(inputs=input, name=f'Model_{net}_numHeads_{heads}', time2vec_dim=1, num_heads=heads, head_size=key_dim, ff_dim=dff, num_layers=layers, dropout=dropout_rate, classes=(len(clip_time)+1))\n","\n","      myModel.compile(\n","          optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","          loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","          metrics=[keras.metrics.SparseCategoricalAccuracy()]\n","      )\n","\n","      callbacks = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=0)\n","\n","      # EarlyStopping criteria\n","      early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n","\n","      # history = myModel.fit(train_batch, validation_data=val_batch, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callbacks, early_stopping])\n","      history = myModel.fit(train_batch, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callbacks]) # without val for offset\n","\n","      # results directory\n","      RES_DIR = f'/content/drive/MyDrive/Colab Notebooks/Final project/Experiments/results/encoder/{net}'\n","      if not os.path.exists(RES_DIR):\n","          os.makedirs(RES_DIR)\n","\n","      # summarize history for accuracy\n","      fig = plt.figure()\n","      plt.plot(history.history['sparse_categorical_accuracy'])\n","      # plt.plot(history.history['val_sparse_categorical_accuracy'])\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n accuracy')\n","      plt.ylabel('accuracy')\n","      plt.xlabel('epoch')\n","      # plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","      fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} accuracy', dpi=fig.dpi)\n","\n","      # summarize history for loss\n","      fig = plt.figure()\n","      plt.plot(history.history['loss'])\n","      # plt.plot(history.history['val_loss'])\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n loss')\n","      plt.ylabel('loss')\n","      plt.xlabel('epoch')\n","      # plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","      fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} loss', dpi=fig.dpi)\n","\n","      ## val Cmat\n","      cm, acc = GetCmat(myModel, networksDict[net]['val'], y_val)\n","      fig = plt.figure()\n","      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","      disp.plot()\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n val acc: {acc:.5}')\n","      plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat val', dpi=fig.dpi)\n","      plt.show()\n","\n","      ## test Cmat\n","      cm, acc = GetCmat(myModel, networksDict[net]['test'], y_test)\n","      fig = plt.figure()\n","      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","      disp.plot()\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n test acc: {acc:.5}')\n","      plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat test', dpi=fig.dpi)\n","      plt.show()\n","\n","\n","      results, results_prob = Get_Results(args, myModel, networksDict[net]['train'], y_train, train_list, train_len, networksDict[net]['test'], y_test, test_list, test_len, clip_time)\n","\n","      myModel.save(f'/content/drive/MyDrive/Colab Notebooks/Final project/Experiments/models/encoder_models/{net}/Model_{net}_numHeads_{heads}_num_layers_{layers}')\n","\n","      res_path = (RES_DIR + \n","                  '/%s_%d_net_%s' %(args.roi_name, args.roi, net) +\n","                  '_k_layers_%d' %(layers) +\n","                  '_heads_%d_batch_size_%d' %(heads, BATCH_SIZE) +\n","                  '_num_epochs_%d.pkl' %(EPOCHS))\n","\n","      # save results\n","      with open(res_path, 'wb') as f:\n","          pickle.dump([results, results_prob], f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSBU1m5-4YDV"},"outputs":[],"source":["### shuffle statistics\n","\n","import pickle\n","from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n","\n","def make_batches(ds):\n","  return (\n","      ds\n","      .cache()\n","      .shuffle(BUFFER_SIZE)\n","      .batch(BATCH_SIZE)\n","      .prefetch(tf.data.AUTOTUNE))\n","\n","def GetCmat(model, X, y):\n","  y_hat_hold = model.predict(X)\n","  y_hat = np.argmax(y_hat_hold, axis=2)\n","  true_y = y\n","\n","  y_overtime = []\n","  y_hat_overtime = []\n","  for rows_y,rows_y_hat in zip(true_y,y_hat):\n","    values, counts = np.unique(rows_y, return_counts=True)\n","    ind = np.argmax(counts)\n","    if values[ind] == 15:\n","      counts[ind] = 0\n","      ind = np.argmax(counts)\n","    y_overtime.append(values[ind])\n","\n","    values, counts = np.unique(rows_y_hat, return_counts=True)\n","    ind = np.argmax(counts)\n","    if values[ind] == 15:\n","      counts[ind] = 0\n","      ind = np.argmax(counts)\n","    y_hat_overtime.append(values[ind])\n","\n","  cm = confusion_matrix(y_overtime,y_hat_overtime)\n","  acc = accuracy_score(y_overtime,y_hat_overtime)\n","  return cm, acc\n","\n","def printLoop(net, head, layer, i):\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print(f'---------------------------NET: {net}  number of heads: {head} number of layers: {layer} round: {i}--------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","\n","def ShuffleAndOffset(y):\n","  y_shuffle = tf.random.shuffle(y)\n","  y_offset = np.array(y)\n","  for i,_ in enumerate(y):\n","    j = np.where(y[i,:]>0)\n","    y_offset[i,j] += 2\n","    j = np.where(y_offset[i,:]==15)\n","    y_offset[i,j] = 1\n","    j = np.where(y_offset[i,:]==16)\n","    y_offset[i,j] = 2\n","    j = np.where(y_offset[i,:]==17)\n","    y_offset[i,j] = 15\n","  y_offset = tf.convert_to_tensor(y_offset)\n","  return y_shuffle, y_offset\n","\n","y_shuffled_t, y_offset_t = ShuffleAndOffset(y_train)\n","\n","num_layers = [6]\n","num_heads = [1]\n","num_of_samples = 10\n","\n","shuffled_acc = np.zeros((num_of_samples,1))\n","\n","\n","for net, y_train_copy in zip(['fullShuffled'], [y_shuffled_t]): # in zip(['fullShuffled', 'fullOffset'], [y_shuffled_t, y_offset_t]):\n","  for heads in num_heads:\n","    for layers in num_layers:\n","      for i in range(0,num_of_samples):\n","\n","        printLoop(net, heads, layers, i)\n","\n","        train_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['train'],y_train_copy))\n","        val_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['val'],y_val))\n","        input = networksDict[net]['train']\n","        train_batch = make_batches(train_data)\n","        val_batch = make_batches(val_data)\n","\n","        myModel = ModelTrunk(inputs=input, name=f'Model_{net}_numHeads_{heads}', time2vec_dim=1, num_heads=heads, head_size=key_dim, ff_dim=dff, num_layers=layers, dropout=dropout_rate, classes=(len(clip_time)+1))\n","\n","        myModel.compile(\n","            optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","            metrics=[keras.metrics.SparseCategoricalAccuracy()]\n","        )\n","        callbacks = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=0)\n","\n","        # EarlyStopping criteria\n","        early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n","\n","        history = myModel.fit(train_batch, validation_data=val_batch, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callbacks, early_stopping])\n","\n","        # results directory\n","        RES_DIR = f'/content/drive/MyDrive/Colab Notebooks/Final project/Experiments/results/encoder/{net}'\n","        if not os.path.exists(RES_DIR):\n","            os.makedirs(RES_DIR)\n","\n","        # summarize history for accuracy\n","        fig = plt.figure()\n","        plt.plot(history.history['sparse_categorical_accuracy'])\n","        # plt.plot(history.history['val_sparse_categorical_accuracy'])\n","        plt.title(f'{net}: {heads} Heads, {layers} layers\\n accuracy')\n","        plt.ylabel('accuracy')\n","        plt.xlabel('epoch')\n","        # plt.legend(['train', 'val'], loc='upper left')\n","        plt.show()\n","        # fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} accuracy', dpi=fig.dpi)\n","\n","        # summarize history for loss\n","        fig = plt.figure()\n","        plt.plot(history.history['loss'])\n","        # plt.plot(history.history['val_loss'])\n","        plt.title(f'{net}: {heads} Heads, {layers} layers\\n loss')\n","        plt.ylabel('loss')\n","        plt.xlabel('epoch')\n","        # plt.legend(['train', 'val'], loc='upper left')\n","        plt.show()\n","        # fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} loss', dpi=fig.dpi)\n","\n","        ## val Cmat\n","        cm, acc = GetCmat(myModel, networksDict[net]['val'], y_val)\n","        fig = plt.figure()\n","        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","        disp.plot()\n","        plt.title(f'{net}: {heads} Heads, {layers} layers\\n val acc: {acc:.5}')\n","        # plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat val', dpi=fig.dpi)\n","        plt.show()\n","\n","        ## test Cmat\n","        cm, acc = GetCmat(myModel, networksDict[net]['test'], y_test)\n","        fig = plt.figure()\n","        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","        disp.plot()\n","        plt.title(f'{net}: {heads} Heads, {layers} layers\\n test acc: {acc:.5}')\n","        # plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat test', dpi=fig.dpi)\n","        plt.show()\n","        shuffled_acc[i] = acc\n","\n","        # results, results_prob = Get_Results(args, myModel, networksDict[net]['train'], y_train, train_list, train_len, networksDict[net]['test'], y_test, test_list, test_len, clip_time)\n","\n","        # myModel.save(f'/content/drive/MyDrive/Colab Notebooks/Final project/Experiments/models/encoder_models/{net}/Model_{net}_numHeads_{heads}_num_layers_{layers}')\n","        \n","      res_path = (RES_DIR + \n","                  '/%s_%d_net_%s' %(args.roi_name, args.roi, net) +\n","                  '_k_layers_%d' %(layers) +\n","                  '_heads_%d_batch_size_%d' %(heads, BATCH_SIZE) +\n","                  '_num_epochs_%d_shuffled_statistics.npy' %(EPOCHS))\n","\n","      # save results\n","      np.save(res_path, shuffled_acc)"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"encoder_for_networks.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
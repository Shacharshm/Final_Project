{"cells":[{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2441,"status":"ok","timestamp":1667822027342,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-120},"id":"NHVVJvIuKn-h","outputId":"ec41bb16-d609-42f0-d0c4-ae3704dd30cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"ErsKsbyJzSKs","executionInfo":{"status":"ok","timestamp":1667823392846,"user_tz":-120,"elapsed":271,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"}}},"outputs":[],"source":["import os\n","\n","# get to project's folder\n","dirPath = \"/content/drive/MyDrive/Colab Notebooks/Final project/Experiments\"\n","os.chdir(dirPath)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3368,"status":"ok","timestamp":1667822030992,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-120},"id":"OMsytu0iKtje","outputId":"a92978ae-ba8c-4d47-b7cc-93530d2d6a88"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.18.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n"]}],"source":["!pip install tensorflow-addons"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"WIKUFenjKtmD","executionInfo":{"status":"ok","timestamp":1667823442257,"user_tz":-120,"elapsed":409,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"}}},"outputs":[],"source":["from Data_extraction_transformer import Get_Data\n","from Results import Get_Results\n","\n","import collections\n","import logging\n","#import os already imported in code cell 2\n","import pathlib\n","import re\n","import string\n","import sys\n","import time\n","import math\n","import pickle\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n","\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import backend as K\n","from tensorflow.keras.layers import MultiHeadAttention\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"VJu3ScdoKtpI","executionInfo":{"status":"ok","timestamp":1667823445204,"user_tz":-120,"elapsed":352,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"}}},"outputs":[],"source":["K_SEED = 330\n","\n","class args:\n","\n","  def __init__(self,input_data,roi,net,roi_name,zscore,train_size):\n","    self.input_data = input_data\n","    self.roi = roi\n","    self.net = net\n","    self.roi_name = roi_name\n","    self.K_RUNS = K_RUNS\n","    # preprocessing\n","    self.zscore = zscore\n","    # training parameters\n","    self.train_size = train_size\n","\n","# data parameters\n","args.input_data = 'data/roi_ts'\n","args.roi = 300\n","args.net = 7\n","args.roi_name = 'roi'\n","args.K_RUNS = 4\n","# preprocessing\n","args.zscore = 1\n","# training parameters\n","args.train_size = 100"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":281,"status":"ok","timestamp":1667823448077,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-120},"id":"z9zuc5ihpBVi"},"outputs":[],"source":["#utils functions\n","def _get_clip_labels():\n","    '''\n","    assign all clips within runs a label\n","    use 0 for testretest\n","    '''\n","    # where are the clips within the run?\n","    timing_file = pd.read_csv('data/videoclip_tr_lookup.csv')\n","\n","    clips = []\n","    for run in range(args.K_RUNS):\n","        run_name = 'MOVIE%d' %(run+1) #MOVIEx_7T_yz\n","        timing_df = timing_file[timing_file['run'].str.contains(run_name)]  \n","        timing_df = timing_df.reset_index(drop=True)\n","\n","        for jj, row in timing_df.iterrows():\n","            clips.append(row['clip_name'])\n","            \n","    clip_y = {}\n","    jj = 1\n","    for clip in clips:\n","        if 'testretest' in clip:\n","            clip_y[clip] = 0\n","        else:\n","            clip_y[clip] = jj\n","            jj += 1\n","\n","    return clip_y\n","\n","\n","def make_batches_train(ds):\n","  return (\n","      ds\n","      .cache()\n","      .shuffle(BUFFER_SIZE)\n","      .batch(BATCH_SIZE)\n","      .prefetch(tf.data.AUTOTUNE))\n","  \n","def make_batches_test(ds):\n","  return (\n","      ds\n","      .cache()\n","      .batch(BATCH_SIZE)\n","      .prefetch(tf.data.AUTOTUNE))\n","\n","def GetCmat(model, X, y):\n","  '''\n","  get the confusion matrix and accuracy from the model.predict(X)\n","  inputs: model, X-Eager tensor of data, y-labels\n","  outputs: cm- confusion matrix, acc-accuracy\n","  '''\n","\n","  y_hat_hold = model.predict(X)\n","  y_hat = np.argmax(y_hat_hold, axis=2)\n","  true_y = y\n","\n","  y_overtime = []\n","  y_hat_overtime = []\n","  for rows_y,rows_y_hat in zip(true_y,y_hat):\n","    values, counts = np.unique(rows_y, return_counts=True)\n","    ind = np.argmax(counts)\n","    if values[ind] == 15:\n","      counts[ind] = 0\n","      ind = np.argmax(counts)\n","    y_overtime.append(values[ind])\n","\n","    values, counts = np.unique(rows_y_hat, return_counts=True)\n","    ind = np.argmax(counts)\n","    if values[ind] == 15:\n","      counts[ind] = 0\n","      ind = np.argmax(counts)\n","    y_hat_overtime.append(values[ind])\n","\n","  cm = confusion_matrix(y_overtime,y_hat_overtime)\n","  acc = accuracy_score(y_overtime,y_hat_overtime)\n","  return cm, acc\n","\n","def ShuffleAndOffset(y):\n","  '''\n","  shuffles and offsets the labels\n","  input: y: the orderd labels\n","  output: y_shuffle: shuffled labels\n","          y_offset: labels offset by 2\n","  '''\n","  # shuffles y\n","  y_shuffle = tf.random.shuffle(y)\n","  # offsets y by 2(1->3,2->4,...,15->1,16->2,17->15)\n","  y_offset = np.array(y)\n","  for i,_ in enumerate(y):\n","    j = np.where(y[i,:]>0)\n","    y_offset[i,j] += 2\n","    j = np.where(y_offset[i,:]==15)\n","    y_offset[i,j] = 1\n","    j = np.where(y_offset[i,:]==16)\n","    y_offset[i,j] = 2\n","    # 17 is the filler label so it stays the last label which is 15\n","    j = np.where(y_offset[i,:]==17)\n","    y_offset[i,j] = 15\n","  y_offset = tf.convert_to_tensor(y_offset)\n","  return y_shuffle, y_offset\n","\n","def lr_scheduler(epoch, lr, warmup_epochs=13, decay_epochs=100, initial_lr=1e-6, base_lr=1e-4, min_lr=5e-5):\n","    if epoch <= warmup_epochs:\n","        pct = epoch / warmup_epochs\n","        return ((base_lr - initial_lr) * pct) + initial_lr\n","\n","    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n","        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n","        return ((base_lr - min_lr) * pct) + min_lr\n","\n","    return min_lr\n","\n","def printLoop(net, head, layer):\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print(f'---------------------------NET: {net}  number of heads: {head} number of layers: {layer}--------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')"]},{"cell_type":"code","source":["# get clips names\n","\n","clip_y = _get_clip_labels()\n","k_class = len(np.unique(list(clip_y.values())))\n","print('number of classes = %d' %k_class)\n","\n","clip_names = np.zeros(k_class).astype(str)\n","clip_names[0] = 'testretest'\n","for key, item in clip_y.items():\n","    if item!=0:\n","        clip_names[item] = key"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NBXxbOKn5WPf","executionInfo":{"status":"ok","timestamp":1667823451886,"user_tz":-120,"elapsed":255,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"}},"outputId":"1a0dccaf-3e92-4ba2-badd-f3e61c1114da"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["number of classes = 15\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":214602,"status":"ok","timestamp":1667819730879,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-120},"id":"6CmFX2zQKtwP","outputId":"e0aac725-fe8a-4150-fe76-125a867f91e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["---\n","roi\n","---\n","loading run 1/4\n","loading run 2/4\n","loading run 3/4\n","loading run 4/4\n","data loading time: 175.67 seconds\n","number of subjects = 176\n","number of features = 300\n","number of classes = 15\n","seq lengths = [ 84 245 222 189  65 227 260 250 181 186 205 143 233 231 256]\n"]}],"source":["# get the orginized data from Get_Data function in Data_extraction_transformer.py\n","X_train, train_len, y_train, X_val, val_len, y_val, X_test, test_len, y_test, train_list, test_list, clip_time = Get_Data(args)"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":384,"status":"ok","timestamp":1667823456583,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-120},"id":"iDPQndEQCTBD"},"outputs":[],"source":["def GetNetwork(X_train, X_val, X_test,startLH,endLH,startRH,endRH):\n","  '''\n","  sets brain network from right and left hemisphere to X\n","  inputs: X_train,X_val,X_test: Eager tensor with all brain networks\n","          startLH,endLH,startRH,endRH: indices of relevant brain network\n","  outputs: X_train_end,X_val_end,X_test_end: Eager tensor with relevant brain network\n","  '''\n","  X_train_LH = X_train[:,:,startLH:endLH]\n","  X_train_RH = X_train[:,:,startRH:endRH]\n","  X_train_end = tf.concat([X_train_LH, X_train_RH], axis=2)\n","\n","  X_val_LH = X_val[:,:,startLH:endLH]\n","  X_val_RH = X_val[:,:,startRH:endRH]\n","  X_val_end = tf.concat([X_val_LH, X_val_RH], axis=2)\n","\n","  X_test_LH = X_test[:,:,startLH:endLH]\n","  X_test_RH = X_test[:,:,startRH:endRH]\n","  X_test_end = tf.concat([X_test_LH, X_test_RH], axis=2)\n","\n","  return X_train_end, X_val_end, X_test_end\n","\n","networksDict = {'vis':{'startLH':0,'endLH':24,'startRH':151,'endRH':174,'train':[], 'val':[], 'test':[]},\n","                'SomMot':{'startLH':24,'endLH':53,'startRH':174,'endRH':201,'train':[], 'val':[], 'test':[]},\n","                'Attn':{'startLH':53,'endLH':85,'startRH':201,'endRH':237,'train':[], 'val':[], 'test':[]},\n","                'limbic':{'startLH':85,'endLH':95,'startRH':237,'endRH':247,'train':[], 'val':[], 'test':[]},\n","                'Cont':{'startLH':95,'endLH':112,'startRH':247,'endRH':270,'train':[], 'val':[], 'test':[]},\n","                'DMN':{'startLH':112,'endLH':150,'startRH':270,'endRH':300,'train':[], 'val':[], 'test':[]},\n","                'full':{'train':X_train, 'val':X_val, 'test':X_test},\n","                'fullShuffled':{'train':X_train, 'val':X_val, 'test':X_test},\n","                'fullOffset':{'train':X_train, 'val':X_val, 'test':X_test}\n","                }\n","\n","show_shape = False\n","\n","for net in networksDict:\n","  if net == 'full' or net == 'fullShuffled' or net == 'fullOffset': continue\n","  networksDict[net]['train'], networksDict[net]['val'], networksDict[net]['test'] = GetNetwork(X_train, X_val, X_test, networksDict[net]['startLH'], networksDict[net]['endLH'], networksDict[net]['startRH'], networksDict[net]['endRH'])\n","  if show_shape:\n","    print(net + ':')\n","    print('train: '+ str(networksDict[net]['train'].shape))\n","    print('val: '+ str(networksDict[net]['val'].shape))\n","    print('test: '+ str(networksDict[net]['test'].shape))"]},{"cell_type":"markdown","metadata":{"id":"sFKlhtoyJhlJ"},"source":["Encoder build"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"sYEkiGxfdGJo","executionInfo":{"status":"ok","timestamp":1667823614883,"user_tz":-120,"elapsed":288,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"}}},"outputs":[],"source":["# set model as sub class of keras.Model\n","\n","class AttentionBlock(keras.Model):\n","    def __init__(self, name='AttentionBlock', num_heads=2, head_size=128, ff_dim=None, dropout=0, **kwargs):\n","        super().__init__(name=name, **kwargs)\n","\n","        if ff_dim is None:\n","            ff_dim = head_size\n","\n","        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)\n","        self.attention_dropout = keras.layers.Dropout(dropout)\n","        self.attention_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.ff_conv1 = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')\n","        # self.ff_conv2 at build()\n","        self.ff_dropout = keras.layers.Dropout(dropout)\n","        self.ff_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","    def build(self, input_shape):\n","        self.ff_conv2 = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1) \n","\n","    def call(self, inputs):\n","        x, attention_scores = self.attention(inputs, inputs, return_attention_scores=True)\n","        x = self.attention_dropout(x)\n","        x = self.attention_norm(inputs + x)\n","\n","        x = self.ff_conv1(x)\n","        x = self.ff_conv2(x)\n","        x = self.ff_dropout(x)\n","\n","        x = self.ff_norm(inputs + x)\n","        return x, attention_scores\n","\n","class ModelTrunk(keras.Model):\n","      def __init__(self, classes, inputs, name='ModelTrunk', num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0, **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        if ff_dim is None:\n","            ff_dim = head_size\n","        self.dropout = dropout\n","        self.classes = classes\n","        self.attention_layers = [AttentionBlock(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, dropout=dropout) for _ in range(num_layers)]\n","        self.dense_layer = keras.layers.Dense(units = 512, activation = 'relu')\n","        self.dropout_layer = keras.layers.Dropout(dropout)\n","        self.final_layer = tf.keras.layers.Dense(classes, activation='softmax')\n","\n","        \n","      def call(self, inputs):\n","        x = inputs\n","        for attention_layer in self.attention_layers:\n","            x, attention_scores = attention_layer(x)\n","        x = self.dense_layer(x)\n","        x = self.dropout_layer(x)\n","        x = self.final_layer(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"0UCh_n-6dGSN","executionInfo":{"status":"ok","timestamp":1667823521838,"user_tz":-120,"elapsed":263,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"}}},"outputs":[],"source":["# set hyperparameters\n","EPOCHS = 45\n","num_layers = [4, 6, 8]\n","d_model = 300\n","dff = 260\n","key_dim = 260\n","num_heads = [1, 4, 6, 8]\n","dropout_rate = 0.1\n","BUFFER_SIZE = 1800\n","BATCH_SIZE = 64"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"qsBpm0aBeEyL","colab":{"base_uri":"https://localhost:8080/","height":696},"executionInfo":{"status":"error","timestamp":1667823689796,"user_tz":-120,"elapsed":61108,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"}},"outputId":"dceab7de-d9fd-4973-c819-54051f922dc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------------------\n","---------------------------NET: vis  number of heads: 1 number of layers: 4--------------------------\n","-----------------------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------------------\n","-----------------------------------------------------------------------------------------------------\n","Epoch 1/45\n","29/29 [==============================] - 23s 654ms/step - loss: 2.8311 - sparse_categorical_accuracy: 0.0631 - val_loss: 2.7883 - val_sparse_categorical_accuracy: 0.0845 - lr: 1.0000e-06\n","Epoch 2/45\n","29/29 [==============================] - 17s 591ms/step - loss: 2.7359 - sparse_categorical_accuracy: 0.1354 - val_loss: 2.5869 - val_sparse_categorical_accuracy: 0.2783 - lr: 8.6154e-06\n","Epoch 3/45\n","29/29 [==============================] - 17s 583ms/step - loss: 2.5454 - sparse_categorical_accuracy: 0.2855 - val_loss: 2.3638 - val_sparse_categorical_accuracy: 0.3297 - lr: 1.6231e-05\n","Epoch 4/45\n"," 7/29 [======>.......................] - ETA: 11s - loss: 2.4392 - sparse_categorical_accuracy: 0.3109"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-09163539dc3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;31m# results directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# train model and get results\n","for net in networksDict:\n","  if net is 'fullOffset' or net is 'fullShuffled': continue\n","  for heads in num_heads:\n","    for layers in num_layers:\n","      printLoop(net, heads, layers)\n","      train_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['train'],y_train))\n","      val_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['val'],y_val))\n","      input = networksDict[net]['train']\n","      train_batch = make_batches_train(train_data)\n","      val_batch = make_batches_test(val_data)\n","\n","      myModel = ModelTrunk(inputs=input, name=f'Model_{net}_numHeads_{heads}', num_heads=heads, head_size=key_dim, ff_dim=dff, num_layers=layers, dropout=dropout_rate, classes=(len(clip_time)+1))\n","\n","      myModel.compile(\n","          optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","          loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","          metrics=[keras.metrics.SparseCategoricalAccuracy()]\n","      )\n","\n","      # LearningRateScheduler\n","      callbacks = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=0)\n","\n","      # EarlyStopping criteria\n","      early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n","\n","      history = myModel.fit(train_batch, validation_data=val_batch, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callbacks, early_stopping])\n","\n","      # results directory\n","      RES_DIR = f'/content/drive/MyDrive/Colab Notebooks/Final project/Experiments/results/encoder/{net}'\n","      if not os.path.exists(RES_DIR):\n","          os.makedirs(RES_DIR)\n","\n","      # summarize history for accuracy\n","      fig = plt.figure()\n","      plt.plot(history.history['sparse_categorical_accuracy'])\n","      plt.plot(history.history['val_sparse_categorical_accuracy'])\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n accuracy')\n","      plt.ylabel('accuracy')\n","      plt.xlabel('epoch')\n","      plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","      fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} accuracy', dpi=fig.dpi)\n","\n","      # summarize history for loss\n","      fig = plt.figure()\n","      plt.plot(history.history['loss'])\n","      plt.plot(history.history['val_loss'])\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n loss')\n","      plt.ylabel('loss')\n","      plt.xlabel('epoch')\n","      plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","      fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} loss', dpi=fig.dpi)\n","\n","      ## val Cmat\n","      cm, acc = GetCmat(myModel, networksDict[net]['val'], y_val)\n","      fig = plt.figure()\n","      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","      disp.plot()\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n val acc: {acc:.5}')\n","      plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat val', dpi=fig.dpi)\n","      plt.show()\n","\n","      ## test Cmat\n","      cm, acc = GetCmat(myModel, networksDict[net]['test'], y_test)\n","      fig = plt.figure()\n","      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","      disp.plot()\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n test acc: {acc:.5}')\n","      plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat test', dpi=fig.dpi)\n","      plt.show()\n","\n","      # # get results\n","      # results, results_prob = Get_Results(args, myModel, networksDict[net]['train'], y_train, train_list, train_len, networksDict[net]['test'], y_test, test_list, test_len, clip_time)\n","\n","      # myModel.save(f'/content/drive/MyDrive/Colab Notebooks/Final project/Experiments/models/encoder_models/{net}/Model_{net}_numHeads_{heads}_num_layers_{layers}')\n","\n","      # res_path = (RES_DIR + \n","      #             '/%s_%d_net_%s' %(args.roi_name, args.roi, net) +\n","      #             '_k_layers_%d' %(layers) +\n","      #             '_heads_%d_batch_size_%d' %(heads, BATCH_SIZE) +\n","      #             '_num_epochs_%d.pkl' %(EPOCHS))\n","\n","      # # save results\n","      # with open(res_path, 'wb') as f:\n","      #     pickle.dump([results, results_prob], f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jRMxgBj0KkO"},"outputs":[],"source":["# train model and get results on offset and shuffled data\n","\n","y_shuffled_t, y_offset_t = ShuffleAndOffset(y_train)\n","\n","for net, y_train_copy in zip(['fullShuffled', 'fullOffset'], [y_shuffled_t, y_offset_t]):\n","  for heads in num_heads:\n","    for layers in num_layers:\n","      printLoop(net, heads, layers)\n","      train_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['train'],y_train_copy))\n","      val_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['val'],y_val))\n","      input = networksDict[net]['train']\n","      train_batch = make_batches_train(train_data)\n","      val_batch = make_batches_test(val_data)\n","\n","      myModel = ModelTrunk(inputs=input, name=f'Model_{net}_numHeads_{heads}', num_heads=heads, head_size=key_dim, ff_dim=dff, num_layers=layers, dropout=dropout_rate, classes=(len(clip_time)+1))\n","\n","      myModel.compile(\n","          optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","          loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","          metrics=[keras.metrics.SparseCategoricalAccuracy()]\n","      )\n","\n","      callbacks = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=0)\n","\n","      # EarlyStopping criteria\n","      early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n","\n","      # history = myModel.fit(train_batch, validation_data=val_batch, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callbacks, early_stopping])\n","      history = myModel.fit(train_batch, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callbacks]) # without val for offset\n","\n","      # results directory\n","      RES_DIR = f'/content/drive/MyDrive/Colab Notebooks/Final project/Experiments/results/encoder/{net}'\n","      if not os.path.exists(RES_DIR):\n","          os.makedirs(RES_DIR)\n","\n","      # summarize history for accuracy\n","      fig = plt.figure()\n","      plt.plot(history.history['sparse_categorical_accuracy'])\n","      # plt.plot(history.history['val_sparse_categorical_accuracy'])\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n accuracy')\n","      plt.ylabel('accuracy')\n","      plt.xlabel('epoch')\n","      # plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","      fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} accuracy', dpi=fig.dpi)\n","\n","      # summarize history for loss\n","      fig = plt.figure()\n","      plt.plot(history.history['loss'])\n","      # plt.plot(history.history['val_loss'])\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n loss')\n","      plt.ylabel('loss')\n","      plt.xlabel('epoch')\n","      # plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","      fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} loss', dpi=fig.dpi)\n","\n","      ## val Cmat\n","      cm, acc = GetCmat(myModel, networksDict[net]['val'], y_val)\n","      fig = plt.figure()\n","      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","      disp.plot()\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n val acc: {acc:.5}')\n","      plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat val', dpi=fig.dpi)\n","      plt.show()\n","\n","      ## test Cmat\n","      cm, acc = GetCmat(myModel, networksDict[net]['test'], y_test)\n","      fig = plt.figure()\n","      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","      disp.plot()\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n test acc: {acc:.5}')\n","      plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat test', dpi=fig.dpi)\n","      plt.show()\n","\n","      # # get results\n","      # results, results_prob = Get_Results(args, myModel, networksDict[net]['train'], y_train, train_list, train_len, networksDict[net]['test'], y_test, test_list, test_len, clip_time)\n","\n","      # myModel.save(f'/content/drive/MyDrive/Colab Notebooks/Final project/Experiments/models/encoder_models/{net}/Model_{net}_numHeads_{heads}_num_layers_{layers}')\n","\n","      # res_path = (RES_DIR + \n","      #             '/%s_%d_net_%s' %(args.roi_name, args.roi, net) +\n","      #             '_k_layers_%d' %(layers) +\n","      #             '_heads_%d_batch_size_%d' %(heads, BATCH_SIZE) +\n","      #             '_num_epochs_%d.pkl' %(EPOCHS))\n","\n","      # # save results\n","      # with open(res_path, 'wb') as f:\n","      #     pickle.dump([results, results_prob], f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSBU1m5-4YDV"},"outputs":[],"source":["# # shuffle statistics\n","# y_shuffled_t, y_offset_t = ShuffleAndOffset(y_train)\n","\n","# num_layers = [6]\n","# num_heads = [1]\n","# num_of_samples = 10\n","\n","# shuffled_acc = np.zeros((num_of_samples,1))\n","\n","\n","# for net, y_train_copy in zip(['fullShuffled', 'fullOffset'], [y_shuffled_t, y_offset_t]):\n","#   for heads in num_heads:\n","#     for layers in num_layers:\n","#       for i in range(0,num_of_samples):\n","\n","#         printLoop(net, heads, layers, i)\n","\n","#         train_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['train'],y_train_copy))\n","#         val_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['val'],y_val))\n","#         input = networksDict[net]['train']\n","#         train_batch = make_batches_train(train_data)\n","#         val_batch = make_batches_test(val_data)\n","\n","#         myModel = ModelTrunk(inputs=input, name=f'Model_{net}_numHeads_{heads}', time2vec_dim=1, num_heads=heads, head_size=key_dim, ff_dim=dff, num_layers=layers, dropout=dropout_rate, classes=(len(clip_time)+1))\n","\n","#         myModel.compile(\n","#             optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","#             loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","#             metrics=[keras.metrics.SparseCategoricalAccuracy()]\n","#         )\n","#         callbacks = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=0)\n","\n","#         # EarlyStopping criteria\n","#         early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n","\n","#         history = myModel.fit(train_batch, validation_data=val_batch, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callbacks, early_stopping])\n","\n","#         # results directory\n","#         RES_DIR = f'/content/drive/MyDrive/Colab Notebooks/Final project/Experiments/results/encoder/{net}'\n","#         if not os.path.exists(RES_DIR):\n","#             os.makedirs(RES_DIR)\n","\n","#         # summarize history for accuracy\n","#         fig = plt.figure()\n","#         plt.plot(history.history['sparse_categorical_accuracy'])\n","#         # plt.plot(history.history['val_sparse_categorical_accuracy'])\n","#         plt.title(f'{net}: {heads} Heads, {layers} layers\\n accuracy')\n","#         plt.ylabel('accuracy')\n","#         plt.xlabel('epoch')\n","#         # plt.legend(['train', 'val'], loc='upper left')\n","#         plt.show()\n","#         # fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} accuracy', dpi=fig.dpi)\n","\n","#         # summarize history for loss\n","#         fig = plt.figure()\n","#         plt.plot(history.history['loss'])\n","#         # plt.plot(history.history['val_loss'])\n","#         plt.title(f'{net}: {heads} Heads, {layers} layers\\n loss')\n","#         plt.ylabel('loss')\n","#         plt.xlabel('epoch')\n","#         # plt.legend(['train', 'val'], loc='upper left')\n","#         plt.show()\n","#         # fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} loss', dpi=fig.dpi)\n","\n","#         ## val Cmat\n","#         cm, acc = GetCmat(myModel, networksDict[net]['val'], y_val)\n","#         fig = plt.figure()\n","#         disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","#         disp.plot()\n","#         plt.title(f'{net}: {heads} Heads, {layers} layers\\n val acc: {acc:.5}')\n","#         # plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat val', dpi=fig.dpi)\n","#         plt.show()\n","\n","#         ## test Cmat\n","#         cm, acc = GetCmat(myModel, networksDict[net]['test'], y_test)\n","#         fig = plt.figure()\n","#         disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","#         disp.plot()\n","#         plt.title(f'{net}: {heads} Heads, {layers} layers\\n test acc: {acc:.5}')\n","#         # plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat test', dpi=fig.dpi)\n","#         plt.show()\n","#         shuffled_acc[i] = acc\n","\n","#         # results, results_prob = Get_Results(args, myModel, networksDict[net]['train'], y_train, train_list, train_len, networksDict[net]['test'], y_test, test_list, test_len, clip_time)\n","\n","#         # myModel.save(f'/content/drive/MyDrive/Colab Notebooks/Final project/Experiments/models/encoder_models/{net}/Model_{net}_numHeads_{heads}_num_layers_{layers}')\n","        \n","#       res_path = (RES_DIR + \n","#                   '/%s_%d_net_%s' %(args.roi_name, args.roi, net) +\n","#                   '_k_layers_%d' %(layers) +\n","#                   '_heads_%d_batch_size_%d' %(heads, BATCH_SIZE) +\n","#                   '_num_epochs_%d_shuffled_statistics.npy' %(EPOCHS))\n","\n","#       # save results\n","#       np.save(res_path, shuffled_acc)"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
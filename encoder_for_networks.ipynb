{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2441,"status":"ok","timestamp":1667822027342,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-120},"id":"NHVVJvIuKn-h","outputId":"ec41bb16-d609-42f0-d0c4-ae3704dd30cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ErsKsbyJzSKs"},"outputs":[],"source":["import os\n","\n","# get to project's folder\n","dirPath = \"/content/drive/MyDrive/Colab Notebooks/Final project/Experiments\"\n","os.chdir(dirPath)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3368,"status":"ok","timestamp":1667822030992,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-120},"id":"OMsytu0iKtje","outputId":"a92978ae-ba8c-4d47-b7cc-93530d2d6a88"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.18.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n"]}],"source":["!pip install tensorflow-addons"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WIKUFenjKtmD"},"outputs":[],"source":["from Data_extraction_transformer import Get_Data\n","from Results import Get_Results\n","\n","import collections\n","import logging\n","#import os already imported in code cell 2\n","import pathlib\n","import re\n","import string\n","import sys\n","import time\n","import math\n","import pickle\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n","\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import backend as K\n","from tensorflow.keras.layers import MultiHeadAttention\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJu3ScdoKtpI"},"outputs":[],"source":["K_SEED = 330\n","\n","class args:\n","\n","  def __init__(self,input_data,roi,net,roi_name,zscore,train_size):\n","    self.input_data = input_data\n","    self.roi = roi\n","    self.net = net\n","    self.roi_name = roi_name\n","    self.K_RUNS = K_RUNS\n","    # preprocessing\n","    self.zscore = zscore\n","    # training parameters\n","    self.train_size = train_size\n","\n","# data parameters\n","args.input_data = 'data/roi_ts'\n","args.roi = 300\n","args.net = 7\n","args.roi_name = 'roi'\n","args.K_RUNS = 4\n","# preprocessing\n","args.zscore = 1\n","# training parameters\n","args.train_size = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z9zuc5ihpBVi"},"outputs":[],"source":["#utils functions\n","def _get_clip_labels():\n","    '''\n","    assign all clips within runs a label\n","    use 0 for testretest\n","    '''\n","    # where are the clips within the run?\n","    timing_file = pd.read_csv('data/videoclip_tr_lookup.csv')\n","\n","    clips = []\n","    for run in range(args.K_RUNS):\n","        run_name = 'MOVIE%d' %(run+1) #MOVIEx_7T_yz\n","        timing_df = timing_file[timing_file['run'].str.contains(run_name)]  \n","        timing_df = timing_df.reset_index(drop=True)\n","\n","        for jj, row in timing_df.iterrows():\n","            clips.append(row['clip_name'])\n","            \n","    clip_y = {}\n","    jj = 1\n","    for clip in clips:\n","        if 'testretest' in clip:\n","            clip_y[clip] = 0\n","        else:\n","            clip_y[clip] = jj\n","            jj += 1\n","\n","    return clip_y\n","\n","\n","def make_batches_train(ds):\n","  return (\n","      ds\n","      .cache()\n","      .shuffle(BUFFER_SIZE)\n","      .batch(BATCH_SIZE)\n","      .prefetch(tf.data.AUTOTUNE))\n","  \n","def make_batches_test(ds):\n","  return (\n","      ds\n","      .cache()\n","      .batch(BATCH_SIZE)\n","      .prefetch(tf.data.AUTOTUNE))\n","\n","def GetCmat(model, X, y):\n","  '''\n","  get the confusion matrix and accuracy from the model.predict(X)\n","  inputs: model, X-Eager tensor of data, y-labels\n","  outputs: cm- confusion matrix, acc-accuracy\n","  '''\n","\n","  y_hat_hold = model.predict(X)\n","  y_hat = np.argmax(y_hat_hold, axis=2)\n","  true_y = y\n","\n","  y_overtime = []\n","  y_hat_overtime = []\n","  for rows_y,rows_y_hat in zip(true_y,y_hat):\n","    values, counts = np.unique(rows_y, return_counts=True)\n","    ind = np.argmax(counts)\n","    if values[ind] == 15:\n","      counts[ind] = 0\n","      ind = np.argmax(counts)\n","    y_overtime.append(values[ind])\n","\n","    values, counts = np.unique(rows_y_hat, return_counts=True)\n","    ind = np.argmax(counts)\n","    if values[ind] == 15:\n","      counts[ind] = 0\n","      ind = np.argmax(counts)\n","    y_hat_overtime.append(values[ind])\n","\n","  cm = confusion_matrix(y_overtime,y_hat_overtime)\n","  acc = accuracy_score(y_overtime,y_hat_overtime)\n","  return cm, acc\n","\n","def ShuffleAndOffset(y):\n","  '''\n","  shuffles and offsets the labels\n","  input: y: the orderd labels\n","  output: y_shuffle: shuffled labels\n","          y_offset: labels offset by 2\n","  '''\n","  # shuffles y\n","  y_shuffle = tf.random.shuffle(y)\n","  # offsets y by 2(1->3,2->4,...,15->1,16->2,17->15)\n","  y_offset = np.array(y)\n","  for i,_ in enumerate(y):\n","    j = np.where(y[i,:]>0)\n","    y_offset[i,j] += 2\n","    j = np.where(y_offset[i,:]==15)\n","    y_offset[i,j] = 1\n","    j = np.where(y_offset[i,:]==16)\n","    y_offset[i,j] = 2\n","    # 17 is the filler label so it stays the last label which is 15\n","    j = np.where(y_offset[i,:]==17)\n","    y_offset[i,j] = 15\n","  y_offset = tf.convert_to_tensor(y_offset)\n","  return y_shuffle, y_offset\n","\n","def lr_scheduler(epoch, lr, warmup_epochs=13, decay_epochs=100, initial_lr=1e-6, base_lr=1e-4, min_lr=5e-5):\n","    if epoch <= warmup_epochs:\n","        pct = epoch / warmup_epochs\n","        return ((base_lr - initial_lr) * pct) + initial_lr\n","\n","    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n","        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n","        return ((base_lr - min_lr) * pct) + min_lr\n","\n","    return min_lr\n","\n","def printLoop(net, head, layer):\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print(f'---------------------------NET: {net}  number of heads: {head} number of layers: {layer}--------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')"]},{"cell_type":"code","source":["# get clips names\n","\n","clip_y = _get_clip_labels()\n","k_class = len(np.unique(list(clip_y.values())))\n","print('number of classes = %d' %k_class)\n","\n","clip_names = np.zeros(k_class).astype(str)\n","clip_names[0] = 'testretest'\n","for key, item in clip_y.items():\n","    if item!=0:\n","        clip_names[item] = key"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NBXxbOKn5WPf","executionInfo":{"status":"ok","timestamp":1667823451886,"user_tz":-120,"elapsed":255,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"}},"outputId":"1a0dccaf-3e92-4ba2-badd-f3e61c1114da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of classes = 15\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":214602,"status":"ok","timestamp":1667819730879,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-120},"id":"6CmFX2zQKtwP","outputId":"e0aac725-fe8a-4150-fe76-125a867f91e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["---\n","roi\n","---\n","loading run 1/4\n","loading run 2/4\n","loading run 3/4\n","loading run 4/4\n","data loading time: 175.67 seconds\n","number of subjects = 176\n","number of features = 300\n","number of classes = 15\n","seq lengths = [ 84 245 222 189  65 227 260 250 181 186 205 143 233 231 256]\n"]}],"source":["# get the orginized data from Get_Data function in Data_extraction_transformer.py\n","X_train, train_len, y_train, X_val, val_len, y_val, X_test, test_len, y_test, train_list, test_list, clip_time = Get_Data(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iDPQndEQCTBD"},"outputs":[],"source":["def GetNetwork(X_train, X_val, X_test,startLH,endLH,startRH,endRH):\n","  '''\n","  sets brain network from right and left hemisphere to X\n","  inputs: X_train,X_val,X_test: Eager tensor with all brain networks\n","          startLH,endLH,startRH,endRH: indices of relevant brain network\n","  outputs: X_train_end,X_val_end,X_test_end: Eager tensor with relevant brain network\n","  '''\n","  X_train_LH = X_train[:,:,startLH:endLH]\n","  X_train_RH = X_train[:,:,startRH:endRH]\n","  X_train_end = tf.concat([X_train_LH, X_train_RH], axis=2)\n","\n","  X_val_LH = X_val[:,:,startLH:endLH]\n","  X_val_RH = X_val[:,:,startRH:endRH]\n","  X_val_end = tf.concat([X_val_LH, X_val_RH], axis=2)\n","\n","  X_test_LH = X_test[:,:,startLH:endLH]\n","  X_test_RH = X_test[:,:,startRH:endRH]\n","  X_test_end = tf.concat([X_test_LH, X_test_RH], axis=2)\n","\n","  return X_train_end, X_val_end, X_test_end\n","\n","networksDict = {'vis':{'startLH':0,'endLH':24,'startRH':151,'endRH':174,'train':[], 'val':[], 'test':[]},\n","                'SomMot':{'startLH':24,'endLH':53,'startRH':174,'endRH':201,'train':[], 'val':[], 'test':[]},\n","                'Attn':{'startLH':53,'endLH':85,'startRH':201,'endRH':237,'train':[], 'val':[], 'test':[]},\n","                'limbic':{'startLH':85,'endLH':95,'startRH':237,'endRH':247,'train':[], 'val':[], 'test':[]},\n","                'Cont':{'startLH':95,'endLH':112,'startRH':247,'endRH':270,'train':[], 'val':[], 'test':[]},\n","                'DMN':{'startLH':112,'endLH':150,'startRH':270,'endRH':300,'train':[], 'val':[], 'test':[]},\n","                'full':{'train':X_train, 'val':X_val, 'test':X_test},\n","                'fullShuffled':{'train':X_train, 'val':X_val, 'test':X_test},\n","                'fullOffset':{'train':X_train, 'val':X_val, 'test':X_test}\n","                }\n","\n","show_shape = False\n","\n","for net in networksDict:\n","  if net == 'full' or net == 'fullShuffled' or net == 'fullOffset': continue\n","  networksDict[net]['train'], networksDict[net]['val'], networksDict[net]['test'] = GetNetwork(X_train, X_val, X_test, networksDict[net]['startLH'], networksDict[net]['endLH'], networksDict[net]['startRH'], networksDict[net]['endRH'])\n","  if show_shape:\n","    print(net + ':')\n","    print('train: '+ str(networksDict[net]['train'].shape))\n","    print('val: '+ str(networksDict[net]['val'].shape))\n","    print('test: '+ str(networksDict[net]['test'].shape))"]},{"cell_type":"markdown","metadata":{"id":"sFKlhtoyJhlJ"},"source":["Encoder build"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYEkiGxfdGJo"},"outputs":[],"source":["# set model as sub class of keras.Model\n","\n","class AttentionBlock(keras.Model):\n","    def __init__(self, name='AttentionBlock', num_heads=2, head_size=128, ff_dim=None, dropout=0, **kwargs):\n","        super().__init__(name=name, **kwargs)\n","\n","        if ff_dim is None:\n","            ff_dim = head_size\n","\n","        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)\n","        self.attention_dropout = keras.layers.Dropout(dropout)\n","        self.attention_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.ff_conv1 = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')\n","        # self.ff_conv2 at build()\n","        self.ff_dropout = keras.layers.Dropout(dropout)\n","        self.ff_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","    def build(self, input_shape):\n","        self.ff_conv2 = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1) \n","\n","    def call(self, inputs):\n","        x, attention_scores = self.attention(inputs, inputs, return_attention_scores=True)\n","        x = self.attention_dropout(x)\n","        x = self.attention_norm(inputs + x)\n","\n","        x = self.ff_conv1(x)\n","        x = self.ff_conv2(x)\n","        x = self.ff_dropout(x)\n","\n","        x = self.ff_norm(inputs + x)\n","        return x, attention_scores\n","\n","class ModelTrunk(keras.Model):\n","      def __init__(self, classes, inputs, name='ModelTrunk', num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0, **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        if ff_dim is None:\n","            ff_dim = head_size\n","        self.dropout = dropout\n","        self.classes = classes\n","        self.attention_layers = [AttentionBlock(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, dropout=dropout) for _ in range(num_layers)]\n","        self.dense_layer = keras.layers.Dense(units = 512, activation = 'relu')\n","        self.dropout_layer = keras.layers.Dropout(dropout)\n","        self.final_layer = tf.keras.layers.Dense(classes, activation='softmax')\n","\n","        \n","      def call(self, inputs):\n","        x = inputs\n","        for attention_layer in self.attention_layers:\n","            x, attention_scores = attention_layer(x)\n","        x = self.dense_layer(x)\n","        x = self.dropout_layer(x)\n","        x = self.final_layer(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0UCh_n-6dGSN"},"outputs":[],"source":["# set hyperparameters\n","EPOCHS = 45\n","num_layers = [4, 6, 8]\n","d_model = 300\n","dff = 260\n","key_dim = 260\n","num_heads = [1, 4, 6, 8]\n","dropout_rate = 0.1\n","BUFFER_SIZE = 1800\n","BATCH_SIZE = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsBpm0aBeEyL"},"outputs":[],"source":["# train model and get results\n","for net in networksDict:\n","  if net is 'fullOffset' or net is 'fullShuffled': continue\n","  for heads in num_heads:\n","    for layers in num_layers:\n","      printLoop(net, heads, layers)\n","      train_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['train'],y_train))\n","      val_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['val'],y_val))\n","      input = networksDict[net]['train']\n","      train_batch = make_batches_train(train_data)\n","      val_batch = make_batches_test(val_data)\n","\n","      myModel = ModelTrunk(inputs=input, name=f'Model_{net}_numHeads_{heads}', num_heads=heads, head_size=key_dim, ff_dim=dff, num_layers=layers, dropout=dropout_rate, classes=(len(clip_time)+1))\n","\n","      myModel.compile(\n","          optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","          loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","          metrics=[keras.metrics.SparseCategoricalAccuracy()]\n","      )\n","\n","      # LearningRateScheduler\n","      callbacks = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=0)\n","\n","      # EarlyStopping criteria\n","      early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n","\n","      history = myModel.fit(train_batch, validation_data=val_batch, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callbacks, early_stopping])\n","\n","      # results directory\n","      RES_DIR = f'{dirPath}/results/encoder/{net}'\n","      if not os.path.exists(RES_DIR):\n","          os.makedirs(RES_DIR)\n","\n","      # summarize history for accuracy\n","      fig = plt.figure()\n","      plt.plot(history.history['sparse_categorical_accuracy'])\n","      plt.plot(history.history['val_sparse_categorical_accuracy'])\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n accuracy')\n","      plt.ylabel('accuracy')\n","      plt.xlabel('epoch')\n","      plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","      fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} accuracy', dpi=fig.dpi)\n","\n","      # summarize history for loss\n","      fig = plt.figure()\n","      plt.plot(history.history['loss'])\n","      plt.plot(history.history['val_loss'])\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n loss')\n","      plt.ylabel('loss')\n","      plt.xlabel('epoch')\n","      plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","      fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} loss', dpi=fig.dpi)\n","\n","      ## val Cmat\n","      cm, acc = GetCmat(myModel, networksDict[net]['val'], y_val)\n","      fig = plt.figure()\n","      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","      disp.plot()\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n val acc: {acc:.5}')\n","      plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat val', dpi=fig.dpi)\n","      plt.show()\n","\n","      ## test Cmat\n","      cm, acc = GetCmat(myModel, networksDict[net]['test'], y_test)\n","      fig = plt.figure()\n","      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","      disp.plot()\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n test acc: {acc:.5}')\n","      plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat test', dpi=fig.dpi)\n","      plt.show()\n","\n","      # get results\n","      results, results_prob = Get_Results(args, myModel, networksDict[net]['train'], y_train, train_list, train_len, networksDict[net]['test'], y_test, test_list, test_len, clip_time)\n","\n","      myModel.save(f'{dirPath}/models/encoder_models/{net}/Model_{net}_numHeads_{heads}_num_layers_{layers}')\n","\n","      res_path = (RES_DIR + \n","                  '/%s_%d_net_%s' %(args.roi_name, args.roi, net) +\n","                  '_k_layers_%d' %(layers) +\n","                  '_heads_%d_batch_size_%d' %(heads, BATCH_SIZE) +\n","                  '_num_epochs_%d.pkl' %(EPOCHS))\n","\n","      # save results\n","      with open(res_path, 'wb') as f:\n","          pickle.dump([results, results_prob], f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jRMxgBj0KkO"},"outputs":[],"source":["# train model and get results on offset and shuffled data\n","\n","y_shuffled_t, y_offset_t = ShuffleAndOffset(y_train)\n","\n","for net, y_train_copy in zip(['fullShuffled', 'fullOffset'], [y_shuffled_t, y_offset_t]):\n","  for heads in num_heads:\n","    for layers in num_layers:\n","      printLoop(net, heads, layers)\n","      train_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['train'],y_train_copy))\n","      val_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['val'],y_val))\n","      input = networksDict[net]['train']\n","      train_batch = make_batches_train(train_data)\n","      val_batch = make_batches_test(val_data)\n","\n","      myModel = ModelTrunk(inputs=input, name=f'Model_{net}_numHeads_{heads}', num_heads=heads, head_size=key_dim, ff_dim=dff, num_layers=layers, dropout=dropout_rate, classes=(len(clip_time)+1))\n","\n","      myModel.compile(\n","          optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","          loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","          metrics=[keras.metrics.SparseCategoricalAccuracy()]\n","      )\n","\n","      callbacks = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=0)\n","\n","      # EarlyStopping criteria\n","      early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n","\n","      # history = myModel.fit(train_batch, validation_data=val_batch, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callbacks, early_stopping])\n","      history = myModel.fit(train_batch, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callbacks]) # without val for offset\n","\n","      # results directory\n","      RES_DIR = f'{dirPath}/results/encoder/{net}'\n","      if not os.path.exists(RES_DIR):\n","          os.makedirs(RES_DIR)\n","\n","      # summarize history for accuracy\n","      fig = plt.figure()\n","      plt.plot(history.history['sparse_categorical_accuracy'])\n","      # plt.plot(history.history['val_sparse_categorical_accuracy'])\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n accuracy')\n","      plt.ylabel('accuracy')\n","      plt.xlabel('epoch')\n","      # plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","      fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} accuracy', dpi=fig.dpi)\n","\n","      # summarize history for loss\n","      fig = plt.figure()\n","      plt.plot(history.history['loss'])\n","      # plt.plot(history.history['val_loss'])\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n loss')\n","      plt.ylabel('loss')\n","      plt.xlabel('epoch')\n","      # plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","      fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} loss', dpi=fig.dpi)\n","\n","      ## val Cmat\n","      cm, acc = GetCmat(myModel, networksDict[net]['val'], y_val)\n","      fig = plt.figure()\n","      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","      disp.plot()\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n val acc: {acc:.5}')\n","      plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat val', dpi=fig.dpi)\n","      plt.show()\n","\n","      ## test Cmat\n","      cm, acc = GetCmat(myModel, networksDict[net]['test'], y_test)\n","      fig = plt.figure()\n","      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","      disp.plot()\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n test acc: {acc:.5}')\n","      plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat test', dpi=fig.dpi)\n","      plt.show()\n","\n","      # get results\n","      results, results_prob = Get_Results(args, myModel, networksDict[net]['train'], y_train, train_list, train_len, networksDict[net]['test'], y_test, test_list, test_len, clip_time)\n","\n","      myModel.save(f'{dirPath}/models/encoder_models/{net}/Model_{net}_numHeads_{heads}_num_layers_{layers}')\n","\n","      res_path = (RES_DIR + \n","                  '/%s_%d_net_%s' %(args.roi_name, args.roi, net) +\n","                  '_k_layers_%d' %(layers) +\n","                  '_heads_%d_batch_size_%d' %(heads, BATCH_SIZE) +\n","                  '_num_epochs_%d.pkl' %(EPOCHS))\n","\n","      # save results\n","      with open(res_path, 'wb') as f:\n","          pickle.dump([results, results_prob], f)"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21122,"status":"ok","timestamp":1651045409786,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-180},"id":"NHVVJvIuKn-h","outputId":"e7fee308-0ce1-4aa2-8148-1fcbf84b3d2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ErsKsbyJzSKs"},"outputs":[],"source":["import os\n","\n","## shachar\n","dirPath = \"/content/drive/MyDrive/Colab Notebooks/Final project/Experiments\"\n","## ariel\n","# dirPath = \"/content/drive/MyDrive/Colab Notebooks/Experiments\"\n","\n","os.chdir(dirPath)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3807,"status":"ok","timestamp":1651045414477,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-180},"id":"OMsytu0iKtje","outputId":"e36be9ba-34b2-41f4-eaec-84b39fd380f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[?25l\r\u001b[K     |▎                               | 10 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 24.6 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 450 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 460 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 471 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 481 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 501 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 512 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 522 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 532 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 542 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 552 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 563 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 573 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 583 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 604 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 614 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 624 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 634 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 645 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 655 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 675 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 686 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 696 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 706 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 716 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 727 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 737 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 747 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 757 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 768 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 778 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 788 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 798 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 808 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 819 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 829 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 849 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 860 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 870 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 880 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 890 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 901 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 911 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 921 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 931 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 942 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 952 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 962 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 972 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 983 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 993 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.16.1\n"]}],"source":["!pip install tensorflow-addons"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WIKUFenjKtmD"},"outputs":[],"source":["from Data_extraction_transformer import Get_Data\n","from Results import Get_Results\n","\n","import collections\n","import logging\n","import os\n","import pathlib\n","import re\n","import string\n","import sys\n","import time\n","import math\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import backend as K\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJu3ScdoKtpI"},"outputs":[],"source":["K_SEED = 330\n","\n","class args:\n","\n","  def __init__(self,input_data,roi,net,roi_name,zscore,train_size):\n","    self.input_data = input_data\n","    self.roi = roi\n","    self.net = net\n","    self.roi_name = roi_name\n","    self.K_RUNS = K_RUNS\n","    # preprocessing\n","    self.zscore = zscore\n","    # training parameters\n","    self.train_size = train_size\n","    \n","\n","# data parameters\n","args.input_data = 'data/roi_ts'\n","args.roi = 300\n","args.net = 7\n","args.roi_name = 'roi'\n","args.K_RUNS = 4\n","# preprocessing\n","args.zscore = 1\n","# training parameters\n","args.train_size = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":439,"status":"ok","timestamp":1651045422694,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-180},"id":"z9zuc5ihpBVi","outputId":"50f70a2b-411c-4719-a5dd-b93976fd7f3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["number of classes = 15\n"]}],"source":["def _get_clip_labels():\n","    '''\n","    assign all clips within runs a label\n","    use 0 for testretest\n","    '''\n","    # where are the clips within the run?\n","    timing_file = pd.read_csv('data/videoclip_tr_lookup.csv')\n","\n","    clips = []\n","    for run in range(args.K_RUNS):\n","        run_name = 'MOVIE%d' %(run+1) #MOVIEx_7T_yz\n","        timing_df = timing_file[timing_file['run'].str.contains(run_name)]  \n","        timing_df = timing_df.reset_index(drop=True)\n","\n","        for jj, row in timing_df.iterrows():\n","            clips.append(row['clip_name'])\n","            \n","    clip_y = {}\n","    jj = 1\n","    for clip in clips:\n","        if 'testretest' in clip:\n","            clip_y[clip] = 0\n","        else:\n","            clip_y[clip] = jj\n","            jj += 1\n","\n","    return clip_y\n","\n","clip_y = _get_clip_labels()\n","k_class = len(np.unique(list(clip_y.values())))\n","print('number of classes = %d' %k_class)\n","\n","clip_names = np.zeros(k_class).astype(str)\n","clip_names[0] = 'testretest'\n","for key, item in clip_y.items():\n","    if item!=0:\n","        clip_names[item] = key"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":206963,"status":"ok","timestamp":1651045629654,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-180},"id":"6CmFX2zQKtwP","outputId":"77f6d41b-6fdd-44d1-d0c5-169009a43372"},"outputs":[{"output_type":"stream","name":"stdout","text":["---\n","roi\n","---\n","1\n","1\n","---\n","Number of hyperparameter combinations: 1\n","---\n","---\n","roi\n","---\n","loading run 1/4\n","loading run 2/4\n","loading run 3/4\n","loading run 4/4\n","data loading time: 167.24 seconds\n","number of subjects = 176\n","number of features = 300\n","number of classes = 15\n","seq lengths = [ 84 245 222 189  65 227 260 250 181 186 205 143 233 231 256]\n"]}],"source":["X_train, train_len, y_train, X_val, val_len, y_val, X_test, test_len, y_test, train_list, test_list, clip_time = Get_Data(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1651045630078,"user":{"displayName":"Shachar Shmueli","userId":"13712970895739115524"},"user_tz":-180},"id":"iDPQndEQCTBD","outputId":"0ad0472a-1096-4ed0-bc9c-29b1a330732d"},"outputs":[{"output_type":"stream","name":"stdout","text":["vis:\n","train: (1800, 260, 47)\n","val: (684, 260, 47)\n","test: (684, 260, 47)\n","SomMot:\n","train: (1800, 260, 56)\n","val: (684, 260, 56)\n","test: (684, 260, 56)\n","Attn:\n","train: (1800, 260, 68)\n","val: (684, 260, 68)\n","test: (684, 260, 68)\n","limbic:\n","train: (1800, 260, 20)\n","val: (684, 260, 20)\n","test: (684, 260, 20)\n","Cont:\n","train: (1800, 260, 40)\n","val: (684, 260, 40)\n","test: (684, 260, 40)\n","DMN:\n","train: (1800, 260, 68)\n","val: (684, 260, 68)\n","test: (684, 260, 68)\n"]}],"source":["def GetNetwork(X_train, X_val, X_test,startLH,endLH,startRH,endRH):\n","  X_train_LH = X_train[:,:,startLH:endLH]\n","  X_train_RH = X_train[:,:,startRH:endRH]\n","  X_train_end = tf.concat([X_train_LH, X_train_RH], axis=2)\n","\n","  X_val_LH = X_val[:,:,startLH:endLH]\n","  X_val_RH = X_val[:,:,startRH:endRH]\n","  X_val_end = tf.concat([X_val_LH, X_val_RH], axis=2)\n","\n","  X_test_LH = X_test[:,:,startLH:endLH]\n","  X_test_RH = X_test[:,:,startRH:endRH]\n","  X_test_end = tf.concat([X_test_LH, X_test_RH], axis=2)\n","\n","  return X_train_end, X_val_end, X_test_end\n","\n","networksDict = {'vis':{'startLH':0,'endLH':24,'startRH':151,'endRH':174,'train':[], 'val':[], 'test':[]},\n","                'SomMot':{'startLH':24,'endLH':53,'startRH':174,'endRH':201,'train':[], 'val':[], 'test':[]},\n","                'Attn':{'startLH':53,'endLH':85,'startRH':201,'endRH':237,'train':[], 'val':[], 'test':[]},\n","                'limbic':{'startLH':85,'endLH':95,'startRH':237,'endRH':247,'train':[], 'val':[], 'test':[]},\n","                'Cont':{'startLH':95,'endLH':112,'startRH':247,'endRH':270,'train':[], 'val':[], 'test':[]},\n","                'DMN':{'startLH':112,'endLH':150,'startRH':270,'endRH':300,'train':[], 'val':[], 'test':[]},\n","                'full':{'train':X_train, 'val':X_val, 'test':X_test},\n","                'fullShuffled':{'train':X_train, 'val':X_val, 'test':X_test},\n","                'fullOffset':{'train':X_train, 'val':X_val, 'test':X_test}\n","                }\n","\n","for net in networksDict:\n","  if net == 'full' or net == 'fullShuffled' or net == 'fullOffset': continue\n","  networksDict[net]['train'], networksDict[net]['val'], networksDict[net]['test'] = GetNetwork(X_train, X_val, X_test, networksDict[net]['startLH'], networksDict[net]['endLH'], networksDict[net]['startRH'], networksDict[net]['endRH'])\n","  print(net + ':')\n","  print('train: '+ str(networksDict[net]['train'].shape))\n","  print('val: '+ str(networksDict[net]['val'].shape))\n","  print('test: '+ str(networksDict[net]['test'].shape))"]},{"cell_type":"markdown","metadata":{"id":"sFKlhtoyJhlJ"},"source":["Encoder build"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYEkiGxfdGJo"},"outputs":[],"source":["from tensorflow.keras.layers import MultiHeadAttention\n","\n","def lr_scheduler(epoch, lr, warmup_epochs=13, decay_epochs=100, initial_lr=1e-6, base_lr=1e-4, min_lr=5e-5):\n","    if epoch <= warmup_epochs:\n","        pct = epoch / warmup_epochs\n","        return ((base_lr - initial_lr) * pct) + initial_lr\n","\n","    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n","        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n","        return ((base_lr - min_lr) * pct) + min_lr\n","\n","    return min_lr\n","\n","\n","class Time2Vec(keras.layers.Layer):\n","    def __init__(self, kernel_size=1):\n","        super(Time2Vec, self).__init__(trainable=True, name='Time2VecLayer')\n","        self.k = kernel_size\n","    \n","    def build(self, input_shape):\n","        # trend\n","        self.wb = self.add_weight(name='wb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n","        self.bb = self.add_weight(name='bb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n","        # periodic\n","        self.wa = self.add_weight(name='wa',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n","        self.ba = self.add_weight(name='ba',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n","        super(Time2Vec, self).build(input_shape)\n","    \n","    def call(self, inputs, **kwargs):\n","        bias = self.wb * inputs + self.bb\n","        dp = K.dot(inputs, self.wa) + self.ba\n","        wgts = K.sin(dp) # or K.cos(.)\n","\n","        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n","        ret = K.reshape(ret, (-1, inputs.shape[1]*(self.k+1)))\n","        return ret\n","    \n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], input_shape[1]*(self.k + 1))\n","\n","class AttentionBlock(keras.Model):\n","    def __init__(self, name='AttentionBlock', num_heads=2, head_size=128, ff_dim=None, dropout=0, **kwargs):\n","        super().__init__(name=name, **kwargs)\n","\n","        if ff_dim is None:\n","            ff_dim = head_size\n","\n","        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)\n","        self.attention_dropout = keras.layers.Dropout(dropout)\n","        self.attention_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.ff_conv1 = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')\n","        # self.ff_conv2 at build()\n","        self.ff_dropout = keras.layers.Dropout(dropout)\n","        self.ff_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.supports_masking = True\n","\n","    def build(self, input_shape):\n","        self.ff_conv2 = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1) \n","\n","    def call(self, inputs):\n","\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","\n","        #if mask is not None:\n","        #    padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n","        #    combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n","        #    combined_mask = tf.minimum(combined_mask, causal_mask)\n","\n","        x, attention_scores = self.attention(inputs, inputs, attention_mask=causal_mask, return_attention_scores=True)\n","        x = self.attention_dropout(x)\n","        x = self.attention_norm(inputs + x)\n","\n","        x = self.ff_conv1(x)\n","        x = self.ff_conv2(x)\n","        x = self.ff_dropout(x)\n","\n","        x = self.ff_norm(inputs + x)\n","        return x, attention_scores\n","\n","    def get_causal_attention_mask(self, inputs):\n","      input_shape = tf.shape(inputs)\n","      batch_size, sequence_length = input_shape[0], input_shape[1]\n","      i = tf.range(sequence_length)[:, tf.newaxis]\n","      j = tf.range(sequence_length)\n","      mask = tf.cast(i >= j, dtype=\"int32\")\n","      mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","      mult = tf.concat(\n","          [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","          axis=0,\n","      )\n","      return tf.tile(mask, mult)\n","\n","class ModelTrunk(keras.Model):\n","      def __init__(self, classes, inputs, name='ModelTrunk', time2vec_dim=1, num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0, **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.MaskingLayer = keras.layers.Masking(mask_value=0.0, input_shape = [None, inputs.shape[-1]])\n","        self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n","        if ff_dim is None:\n","            ff_dim = head_size\n","        self.dropout = dropout\n","        self.classes = classes\n","        self.attention_layers = [AttentionBlock(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, dropout=dropout) for _ in range(num_layers)]\n","        self.dense_layer = keras.layers.Dense(units = 512, activation = 'relu')\n","        self.dropout_layer = keras.layers.Dropout(dropout)\n","        self.final_layer = tf.keras.layers.Dense(classes, activation='softmax')\n","\n","        \n","      def call(self, inputs):\n","        #time_embedding = keras.layers.TimeDistributed(self.time2vec)(inputs)\n","        #x = K.concatenate([inputs, time_embedding], -1)\n","        #x = MaskingLayer(inputs)\n","        x = inputs\n","        for attention_layer in self.attention_layers:\n","            x, attention_scores = attention_layer(x)\n","        x = self.dense_layer(x)\n","        x = self.dropout_layer(x)\n","        x = self.final_layer(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0UCh_n-6dGSN"},"outputs":[],"source":["EPOCHS = 45\n","num_layers = [4, 6, 8]\n","d_model = 300\n","dff = 260\n","key_dim = 260\n","num_heads = [1, 4, 6, 8]\n","dropout_rate = 0.1\n","BUFFER_SIZE = 20000\n","BATCH_SIZE = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsBpm0aBeEyL"},"outputs":[],"source":["import pickle\n","from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n","\n","def make_batches(ds):\n","  return (\n","      ds\n","      .cache()\n","      .shuffle(BUFFER_SIZE)\n","      .batch(BATCH_SIZE)\n","      .prefetch(tf.data.AUTOTUNE))\n","\n","def GetCmat(model, X, y):\n","  y_hat_hold = model.predict(X)\n","  y_hat = np.argmax(y_hat_hold, axis=2)\n","  true_y = y\n","\n","  y_overtime = []\n","  y_hat_overtime = []\n","  for rows_y,rows_y_hat in zip(true_y,y_hat):\n","    values, counts = np.unique(rows_y, return_counts=True)\n","    ind = np.argmax(counts)\n","    if values[ind] == 15:\n","      counts[ind] = 0\n","      ind = np.argmax(counts)\n","    y_overtime.append(values[ind])\n","\n","    values, counts = np.unique(rows_y_hat, return_counts=True)\n","    ind = np.argmax(counts)\n","    if values[ind] == 15:\n","      counts[ind] = 0\n","      ind = np.argmax(counts)\n","    y_hat_overtime.append(values[ind])\n","\n","  cm = confusion_matrix(y_overtime,y_hat_overtime)\n","  acc = accuracy_score(y_overtime,y_hat_overtime)\n","  return cm, acc\n","\n","def printLoop(net, head, layer):\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print(f'---------------------------NET: {net}  number of heads: {head} number of layers: {layer}--------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","\n","\n","for net in networksDict:\n","  if net == 'fullOffset' or net == 'fullShuffled': continue\n","  if net is not 'full': continue\n","  for heads in num_heads:\n","    for layers in num_layers:\n","      printLoop(net, heads, layers)\n","      train_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['train'],y_train))\n","      val_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['val'],y_val))\n","      input = networksDict[net]['train']\n","      train_batch = make_batches(train_data)\n","      val_batch = make_batches(val_data)\n","\n","      myModel = ModelTrunk(inputs=input, name=f'Model_{net}_numHeads_{heads}', time2vec_dim=1, num_heads=heads, head_size=key_dim, ff_dim=dff, num_layers=layers, dropout=dropout_rate, classes=(len(clip_time)+1))\n","\n","      myModel.compile(\n","          optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","          loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","          metrics=[keras.metrics.SparseCategoricalAccuracy()]\n","      )\n","\n","      callbacks = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=0)\n","\n","      # EarlyStopping criteria\n","      early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n","\n","      history = myModel.fit(train_batch, validation_data=val_batch, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callbacks, early_stopping])\n","\n","      # results directory\n","      RES_DIR = f'/content/drive/MyDrive/Colab Notebooks/Final project/Experiments/results/decoder/{net}'\n","      if not os.path.exists(RES_DIR):\n","          os.makedirs(RES_DIR)\n","\n","      # summarize history for accuracy\n","      fig = plt.figure()\n","      plt.plot(history.history['sparse_categorical_accuracy'])\n","      plt.plot(history.history['val_sparse_categorical_accuracy'])\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n accuracy')\n","      plt.ylabel('accuracy')\n","      plt.xlabel('epoch')\n","      plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","      fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} accuracy', dpi=fig.dpi)\n","\n","      # summarize history for loss\n","      fig = plt.figure()\n","      plt.plot(history.history['loss'])\n","      plt.plot(history.history['val_loss'])\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n loss')\n","      plt.ylabel('loss')\n","      plt.xlabel('epoch')\n","      plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","      fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} loss', dpi=fig.dpi)\n","\n","      ## val Cmat\n","      cm, acc = GetCmat(myModel, networksDict[net]['val'], y_val)\n","      fig = plt.figure()\n","      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","      disp.plot()\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n val acc: {acc:.5}')\n","      plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat val', dpi=fig.dpi)\n","      plt.show()\n","\n","      ## test Cmat\n","      cm, acc = GetCmat(myModel, networksDict[net]['test'], y_test)\n","      fig = plt.figure()\n","      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","      disp.plot()\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n test acc: {acc:.5}')\n","      plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat test', dpi=fig.dpi)\n","      plt.show()\n","\n","\n","      results, results_prob = Get_Results(args, myModel, networksDict[net]['train'], y_train, train_list, train_len, networksDict[net]['test'], y_test, test_list, test_len, clip_time)\n","\n","      myModel.save(f'/content/drive/MyDrive/Colab Notebooks/Final project/Experiments/models/decoder_models/{net}/Model_{net}_numHeads_{heads}_num_layers_{layers}')\n","\n","      res_path = (RES_DIR + \n","                  '/%s_%d_net_%s' %(args.roi_name, args.roi, net) +\n","                  '_k_layers_%d' %(layers) +\n","                  '_heads_%d_batch_size_%d' %(heads, BATCH_SIZE) +\n","                  '_num_epochs_%d.pkl' %(EPOCHS))\n","\n","      # save results\n","      with open(res_path, 'wb') as f:\n","          pickle.dump([results, results_prob], f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jRMxgBj0KkO"},"outputs":[],"source":["### offset and shuffled\n","\n","import pickle\n","from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n","\n","def make_batches(ds):\n","  return (\n","      ds\n","      .cache()\n","      .shuffle(BUFFER_SIZE)\n","      .batch(BATCH_SIZE)\n","      .prefetch(tf.data.AUTOTUNE))\n","\n","def GetCmat(model, X, y):\n","  y_hat_hold = model.predict(X)\n","  y_hat = np.argmax(y_hat_hold, axis=2)\n","  true_y = y\n","\n","  y_overtime = []\n","  y_hat_overtime = []\n","  for rows_y,rows_y_hat in zip(true_y,y_hat):\n","    values, counts = np.unique(rows_y, return_counts=True)\n","    ind = np.argmax(counts)\n","    if values[ind] == 15:\n","      counts[ind] = 0\n","      ind = np.argmax(counts)\n","    y_overtime.append(values[ind])\n","\n","    values, counts = np.unique(rows_y_hat, return_counts=True)\n","    ind = np.argmax(counts)\n","    if values[ind] == 15:\n","      counts[ind] = 0\n","      ind = np.argmax(counts)\n","    y_hat_overtime.append(values[ind])\n","\n","  cm = confusion_matrix(y_overtime,y_hat_overtime)\n","  acc = accuracy_score(y_overtime,y_hat_overtime)\n","  return cm, acc\n","\n","def printLoop(net, head, layer):\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print(f'---------------------------NET: {net}  number of heads: {head} number of layers: {layer}--------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","  print('-----------------------------------------------------------------------------------------------------')\n","\n","def ShuffleAndOffset(y):\n","  y_shuffle = tf.random.shuffle(y)\n","  y_offset = np.array(y)\n","  for i,_ in enumerate(y):\n","    j = np.where(y[i,:]>0)\n","    y_offset[i,j] += 2\n","    j = np.where(y_offset[i,:]==15)\n","    y_offset[i,j] = 1\n","    j = np.where(y_offset[i,:]==16)\n","    y_offset[i,j] = 2\n","    j = np.where(y_offset[i,:]==17)\n","    y_offset[i,j] = 15\n","  y_offset = tf.convert_to_tensor(y_offset)\n","  return y_shuffle, y_offset\n","\n","y_shuffled_t, y_offset_t = ShuffleAndOffset(y_train)\n","\n","num_layers = [6]\n","num_heads = [1]\n","\n","for net, y_train_copy in zip(['fullOffset'], [y_offset_t]): # in zip(['fullShuffled', 'fullOffset'], [y_shuffled_t, y_offset_t]):\n","  for heads in num_heads:\n","    for layers in num_layers:\n","      printLoop(net, heads, layers)\n","      train_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['train'],y_train_copy))\n","      val_data = tf.data.Dataset.from_tensor_slices((networksDict[net]['val'],y_val))\n","      input = networksDict[net]['train']\n","      train_batch = make_batches(train_data)\n","      val_batch = make_batches(val_data)\n","\n","      myModel = ModelTrunk(inputs=input, name=f'Model_{net}_numHeads_{heads}', time2vec_dim=1, num_heads=heads, head_size=key_dim, ff_dim=dff, num_layers=layers, dropout=dropout_rate, classes=(len(clip_time)+1))\n","\n","      myModel.compile(\n","          optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","          loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n","          metrics=[keras.metrics.SparseCategoricalAccuracy()]\n","      )\n","\n","      callbacks = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=0)\n","\n","      # EarlyStopping criteria\n","      early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n","\n","      # history = myModel.fit(train_batch, validation_data=val_batch, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callbacks, early_stopping])\n","      history = myModel.fit(train_batch, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[callbacks]) # without val for offset\n","\n","      # results directory\n","      RES_DIR = f'/content/drive/MyDrive/Colab Notebooks/Final project/Experiments/results/encoder/{net}'\n","      if not os.path.exists(RES_DIR):\n","          os.makedirs(RES_DIR)\n","\n","      # summarize history for accuracy\n","      fig = plt.figure()\n","      plt.plot(history.history['sparse_categorical_accuracy'])\n","      # plt.plot(history.history['val_sparse_categorical_accuracy'])\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n accuracy')\n","      plt.ylabel('accuracy')\n","      plt.xlabel('epoch')\n","      # plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","      fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} accuracy', dpi=fig.dpi)\n","\n","      # summarize history for loss\n","      fig = plt.figure()\n","      plt.plot(history.history['loss'])\n","      # plt.plot(history.history['val_loss'])\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n loss')\n","      plt.ylabel('loss')\n","      plt.xlabel('epoch')\n","      # plt.legend(['train', 'val'], loc='upper left')\n","      plt.show()\n","      fig.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} loss', dpi=fig.dpi)\n","\n","      ## val Cmat\n","      cm, acc = GetCmat(myModel, networksDict[net]['val'], y_val)\n","      fig = plt.figure()\n","      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","      disp.plot()\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n val acc: {acc:.5}')\n","      plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat val', dpi=fig.dpi)\n","      plt.show()\n","\n","      ## test Cmat\n","      cm, acc = GetCmat(myModel, networksDict[net]['test'], y_test)\n","      fig = plt.figure()\n","      disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","      disp.plot()\n","      plt.title(f'{net}: {heads} Heads, {layers} layers\\n test acc: {acc:.5}')\n","      plt.savefig(RES_DIR + f'/net: {net} numHeads: {heads} layers: {layers} Cmat test', dpi=fig.dpi)\n","      plt.show()\n","\n","\n","      results, results_prob = Get_Results(args, myModel, networksDict[net]['train'], y_train, train_list, train_len, networksDict[net]['test'], y_test, test_list, test_len, clip_time)\n","\n","      myModel.save(f'/content/drive/MyDrive/Colab Notebooks/Final project/Experiments/models/encoder_models/{net}/Model_{net}_numHeads_{heads}_num_layers_{layers}')\n","\n","      res_path = (RES_DIR + \n","                  '/%s_%d_net_%s' %(args.roi_name, args.roi, net) +\n","                  '_k_layers_%d' %(layers) +\n","                  '_heads_%d_batch_size_%d' %(heads, BATCH_SIZE) +\n","                  '_num_epochs_%d.pkl' %(EPOCHS))\n","\n","      # save results\n","      with open(res_path, 'wb') as f:\n","          pickle.dump([results, results_prob], f)"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"decoder_for_networks.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}